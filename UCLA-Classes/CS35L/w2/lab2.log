George Owen
405196467

lab2.log
--------------

This is what I did on the lab assignment:


I changed my locale by using the provided command export "LC_ALL=C"

Next, I obtained the required webpage by using curl -O http://web.cs.ucla.edu/classes/winter20/cs35L/assign/assign2.html

then began formatting the words document. I used "cp /usr/share/dict/words ~/werds" then "sort werds > words" to create a sorted list of words in my local directory.


After this, I was ready to start figuring out what the specified commands do:
tr -c 'A-Za-z' '[\n*]' < assign2.html
		this replaces all characters that are not found in the alphabet (not 'a-z' or 'A-Z') with newlines. Basically, this is an effective way of clearing the formatting of documents
tr -cs 'A-Za-z' '[\n*]' < assign2.html
		this behaves similarly to the previous command (replacing non-alphabetical characters with newlines), but squeezes any repeats so that there's not multiple newlines in a row. 
		This gets rid of excess newlines essentially
tr -cs 'A-Za-z' '[\n*]' < assign2.html | sort
		Once again formats the document to contain only words, but this time sorts the words in alphabetical order as well
tr -cs 'A-Za-z' '[\n*]' < assign2.html | sort -u
		Sorts in the same manner as the previous command, but only lists unique words (duplicates of the same word are not listed)
tr -cs 'A-Za-z' '[\n*]' < assign2.html | sort -u | comm - words
		pipes the sorted list from the previous command to a comm command - this compares our formatted document to the master list found in "words." 
		This outputs three columns: words unique to "assign2.html", words unique to "words", and words shared by both, in that order.
tr -cs 'A-Za-z' '[\n*]' < assign2.html | sort -u | comm -23 words
		makes the same comparison as the last command, but suppresses columns 2 and 3 of comm's output. 
		This means the only words that will be printed are words not found in the "words" doc (aka incorrectly spelled words


-------------------------
Next, I began formatting my hawaiian words doc, using the html file obtained from this command: curl -o hawaiian.html https://www.mauimapp.com/moolelo/hwnwdshw.htm

Here is my script:

#!/bin/bash


#removes all instances of <u>, </u>, and <br> from the file. These are common html tags
sed -E "s/\?|<u>|<\/u>|<br>//g" $1 | #hawaiian.html |

#makes sure all english definitions only take up a single line. Basially finds lines that wrap to a newline, appends the next line to the
#current line, and substitures the newline character with a space
sed -E '/[^>]$/{N; s/\n//}' | 


sed -E '/[^>]$/{N; s/\n//}' |
grep -E '<td.*>.+</td>' |

sed '1,4d' |
sed '2~2d' |
sed -E 's/<td[a-zA-Z\ \=\" ]*>|<\/td>//g' |

tr [\`] [\'] |
tr [A-Z] [a-z] |
tr -s ,\ \- '[\n*]' |

sed -E "/./!d" |

sed -E "s/[^pk\'mnwlhaeiou]//g" |

sort -u #> hwords











./buildwords [filename] - reads in a file and outputs a formatted list of all hawaiian words in that file

tr 'A-Z' 'a-z' < hawaiian.html | tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - hwords  #HAWAIIAN SPELLCHECKER

This command takes in an input file (in this case, hawaiian.html) and spellchecks it. It accomplishes this by first making all letters lowercase, then replaces every nonletter character with a newline. 
After this, it sorts the words alphabetically, and compares it to a formattes list of hawaiian words called "hwords". This hwords file is the file generated by the buildwords script described above.